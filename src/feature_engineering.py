import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.cluster import KMeans
import logging
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class FeatureEngineer:
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        # Convert timestamp immediately to ensure datetime methods work
        self.df['TransactionStartTime'] = pd.to_datetime(self.df['TransactionStartTime'])

    def extract_temporal_features(self):
        """Extracts Hour, Day, Month, Year from TransactionStartTime."""
        try:
            logging.info("Extracting temporal features...")
            self.df['TransactionHour'] = self.df['TransactionStartTime'].dt.hour
            self.df['TransactionDay'] = self.df['TransactionStartTime'].dt.day
            self.df['TransactionMonth'] = self.df['TransactionStartTime'].dt.month
            self.df['TransactionYear'] = self.df['TransactionStartTime'].dt.year
            return self
        except Exception as e:
            logging.error(f"Error extracting temporal features: {e}")
            raise e

    def create_aggregate_features(self):
        """Creates aggregates (Sum, Mean, Count, Std) grouped by CustomerId."""
        try:
            logging.info("Creating aggregate features...")
            # Group by CustomerId
            aggs = self.df.groupby('CustomerId').agg({
                'Amount': ['sum', 'mean', 'count', 'std'],
                'Value': ['sum', 'mean', 'max'] 
            })
            
            # Flatten MultiIndex columns (e.g., 'Amount_sum', 'Amount_mean')
            aggs.columns = ['_'.join(col).strip() for col in aggs.columns.values]
            
            # Reset index to make CustomerId a column
            aggs = aggs.reset_index()
            
            # Merge back to main dataframe
            self.df = self.df.merge(aggs, on='CustomerId', how='left')
            
            # Fill NaNs generated by std() for single transactions
            # (Standard deviation of 1 number is NaN)
            self.df = self.df.fillna(0)
            return self
        except Exception as e:
            logging.error(f"Error creating aggregate features: {e}")
            raise e

    def create_rfm_target(self, n_clusters=3, random_state=42):
        """
        Task 4: Creates RFM features and the Proxy Target Variable using K-Means.
        Target: 0 = Low Risk (Good), 1 = High Risk (Bad)
        """
        try:
            logging.info("Calculating RFM and creating Proxy Target...")
            
            # 1. Calculate RFM
            current_date = self.df['TransactionStartTime'].max()
            rfm = self.df.groupby('CustomerId').agg({
                'TransactionStartTime': lambda x: (current_date - x.max()).days, # Recency
                'TransactionId': 'count',                                        # Frequency
                'Amount': 'sum'                                                  # Monetary
            }).rename(columns={
                'TransactionStartTime': 'Recency',
                'TransactionId': 'Frequency',
                'Amount': 'Monetary'
            })
            
            # 2. Normalize RFM for Clustering (K-Means is sensitive to scale)
            scaler = StandardScaler()
            rfm_scaled = scaler.fit_transform(rfm)
            
            # 3. K-Means Clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
            rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)
            
            # 4. Assign "High Risk" Label
            # Logic: The cluster with the Lowest Frequency is typically the "Bad" (High Risk/Low Engagement) group.
            cluster_summary = rfm.groupby('Cluster').mean()
            high_risk_cluster = cluster_summary['Frequency'].idxmin()
            
            rfm['is_high_risk'] = (rfm['Cluster'] == high_risk_cluster).astype(int)
            
            logging.info(f"High Risk Cluster identified: {high_risk_cluster}")
            logging.info(f"Class distribution: \n{rfm['is_high_risk'].value_counts()}")
            
            # 5. Merge Target back to main DF
            self.df = self.df.merge(rfm[['is_high_risk', 'Recency', 'Frequency', 'Monetary']], on='CustomerId', how='left')
            return self

        except Exception as e:
            logging.error(f"Error creating RFM target: {e}")
            raise e

    def encode_categorical(self):
        """One-Hot Encodes and Label Encodes categorical features."""
        try:
            logging.info("Encoding categorical variables...")
            
            # One-Hot Encoding for ChannelId, ProductCategory, PricingStrategy
            # drop_first=True avoids multicollinearity
            self.df = pd.get_dummies(self.df, columns=['ChannelId', 'ProductCategory', 'PricingStrategy'], drop_first=True)
            
            # Label Encoding for ProviderId (Too many categories for One-Hot)
            le = LabelEncoder()
            self.df['ProviderId'] = le.fit_transform(self.df['ProviderId'].astype(str))
            
            return self
        except Exception as e:
            logging.error(f"Error encoding categorical variables: {e}")
            raise e
            
    def get_processed_data(self):
        """Returns the processed dataframe."""
        return self.df

if __name__ == "__main__":
    # Define paths (dynamically find the project root)
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    DATA_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'data.csv')
    OUTPUT_PATH = os.path.join(BASE_DIR, 'data', 'processed', 'train_data.csv')
    
    # Check if data exists
    if os.path.exists(DATA_PATH):
        print(f"Loading data from {DATA_PATH}...")
        df = pd.read_csv(DATA_PATH)
        
        # Initialize and Run Pipeline
        engineer = FeatureEngineer(df)
        engineer.extract_temporal_features()
        engineer.create_aggregate_features()
        engineer.create_rfm_target()
        engineer.encode_categorical()
        
        # Save Result
        df_processed = engineer.get_processed_data()
        
        # Create processed folder if not exists
        os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
        
        df_processed.to_csv(OUTPUT_PATH, index=False)
        print(f"Success! Processed data saved to {OUTPUT_PATH}")
        print(f"Final Data Shape: {df_processed.shape}")
        
        # Preview Target Distribution
        print("\nTarget Variable (is_high_risk) Distribution:")
        print(df_processed['is_high_risk'].value_counts())
    else:
        print(f"Error: File not found at {DATA_PATH}")